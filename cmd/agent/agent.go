package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"math/rand"
	"os"
	"time"

	"github.com/mark3labs/mcp-go/client"
	"github.com/mark3labs/mcp-go/mcp"
	"github.com/sashabaranov/go-openai"
)

const (
	LLM_ENDPOINT = "https://dashscope.aliyuncs.com/compatible-mode/v1" // é˜¿é‡Œç™¾ç‚¼æœåŠ¡åœ°å€
	MCP_ENDPOINT = "http://localhost:9001/mcp"
	MODEL_NAME   = "qwen-turbo" // é˜¿é‡Œç™¾ç‚¼æä¾›çš„æ¨¡å‹åç§°
)

// æ¨¡æ‹Ÿé‡‡é›†ç³»ç»ŸæŒ‡æ ‡
func getCurrentMetrics() map[string]float64 {
	return map[string]float64{
		"cpu_usage_percent": 80 + rand.Float64()*15, // 80~95%
	}

}

func main() {
	rand.Seed(time.Now().UnixNano())
	log.Println("ğŸ•’ Auto Decision Agent started. Running every 5 seconds...")

	// åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ˆå…¼å®¹ OpenAI APIï¼‰
	apiKey := os.Getenv("LLM_API_KEY")
	if apiKey == "" {
		log.Fatal("è¯·è®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡")
	}
	config := openai.DefaultConfig(apiKey)
	config.BaseURL = LLM_ENDPOINT
	llmClient := openai.NewClientWithConfig(config)

	// åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
	mcpCli, err := client.NewStreamableHttpClient(MCP_ENDPOINT)
	if err != nil {
		log.Fatalf("Failed to create MCP client: %v", err)
	}

	// å¯åŠ¨ MCP å®¢æˆ·ç«¯
	if err := mcpCli.Start(context.Background()); err != nil {
		log.Fatalf("Failed to start MCP client: %v", err)
	}

	// ç­‰å¾…è¿æ¥å»ºç«‹
	time.Sleep(1 * time.Second)

	// æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å·²åˆå§‹åŒ–
	if !mcpCli.IsInitialized() {
		// åˆå§‹åŒ– MCP å®¢æˆ·ç«¯è¿æ¥
		initReq := mcp.InitializeRequest{
			Params: mcp.InitializeParams{
				ProtocolVersion: mcp.LATEST_PROTOCOL_VERSION,
				ClientInfo: mcp.Implementation{
					Name:    "auto-decision-agent",
					Version: "1.0.0",
				},
			},
		}

		_, err = mcpCli.Initialize(context.Background(), initReq)
		if err != nil {
			log.Fatalf("Failed to initialize MCP client: %v", err)
		}
	}

	defer mcpCli.Close()

	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()

	for range ticker.C {
		metrics := getCurrentMetrics()
		cpu := metrics["cpu_usage_percent"]

		prompt := fmt.Sprintf(`
å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š
- CPU ä½¿ç”¨ç‡: %.1f%%

è§„åˆ™ï¼šå¦‚æœ CPU ä½¿ç”¨ç‡æŒç»­é«˜äº 85%%ï¼Œå»ºè®®å°†å‘Šè­¦é˜ˆå€¼é€‚å½“è°ƒé«˜ï¼ˆä¾‹å¦‚ 90ï¼‰ï¼Œä»¥é¿å…é¢‘ç¹å‘Šè­¦ã€‚
è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´é…ç½®ã€‚å¦‚éœ€è°ƒæ•´ï¼Œè¯·è°ƒç”¨ adjust_cpu_threshold å·¥å…·ï¼Œå¹¶ä¼ å…¥æ–°çš„é˜ˆå€¼ï¼ˆæ•°å­—ï¼Œ0~100ï¼‰ã€‚
`, cpu)

		log.Printf("ğŸ“Š å½“å‰ CPU: %.1f%%", cpu)

		// è°ƒç”¨ LLM
		resp, err := llmClient.CreateChatCompletion(
			context.Background(),
			openai.ChatCompletionRequest{
				Model: MODEL_NAME,
				Messages: []openai.ChatCompletionMessage{
					{Role: openai.ChatMessageRoleUser, Content: prompt},
				},
				Tools: []openai.Tool{
					{
						Type: openai.ToolTypeFunction,
						Function: &openai.FunctionDefinition{
							Name:        "adjust_cpu_threshold",
							Description: "è°ƒæ•´ CPU å‘Šè­¦é˜ˆå€¼",
							Parameters: map[string]interface{}{
								"type": "object",
								"properties": map[string]interface{}{
									"value": map[string]string{"type": "number"},
								},
								"required": []string{"value"},
							},
						},
					},
				},
				ToolChoice: "auto",
			},
		)

		if err != nil {
			log.Printf("âŒ LLM è°ƒç”¨å¤±è´¥: %v", err)
			continue
		}

		msg := resp.Choices[0].Message
		if len(msg.ToolCalls) > 0 {
			for _, tc := range msg.ToolCalls {
				if tc.Function.Name == "adjust_cpu_threshold" {
					var args map[string]interface{}
					if err := json.Unmarshal([]byte(tc.Function.Arguments), &args); err != nil {
						log.Printf("âš ï¸ å‚æ•°è§£æå¤±è´¥: %v", err)
						continue
					}

					log.Printf("ğŸ§  LLM å†³ç­–: è°ƒç”¨ %s(%v)", tc.Function.Name, args)

					// è°ƒç”¨ MCP å·¥å…·
					params := mcp.CallToolParams{
						Name:      tc.Function.Name,
						Arguments: args,
					}

					request := mcp.CallToolRequest{
						Params: params,
					}

					result, err := mcpCli.CallTool(context.Background(), request)
					if err != nil {
						log.Printf("âŒ MCP æ‰§è¡Œå¤±è´¥: %v", err)
					} else {
						log.Printf("âœ… é…ç½®å·²æ›´æ–°: %+v", result)
					}
				}
			}
		} else {
			log.Println("ğŸ¤” LLM è®¤ä¸ºæ— éœ€è°ƒæ•´é…ç½®")
		}
	}
}
